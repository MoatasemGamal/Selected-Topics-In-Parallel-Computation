\documentclass[12pt, a4paper]{book}
\usepackage{authblk, fancyhdr, hyperref, multirow, graphicx, subfig, longtable}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\usepackage[left=25mm,right=25mm,top=25mm,bottom=25mm]{geometry}
\usepackage[linguistics]{forest}
\usepackage{listings}
\usepackage{amssymb}

\newcommand\Chapter[2]{
  \chapter[#1: {\itshape#2}]{#1\\[2ex]\Large\itshape#2}
}
\usepackage{lipsum}
\pagestyle{headings}

\title{Parallel Computation Summary}
\author{Moatasem Gamal}
\affil{FCAI BSU}

\begin{document}
\maketitle
\tableofcontents
\chapter{Multithreaded Programming}
\section{Introduction}
\begin{itemize}
    \item Modern operating systems hold more than one activity (program) in memory and the processor can switch among all to execute them.
    \item \textbf{Multitasking/MultiProcesses:} is the simultaneous occurrence of several activities (program) on a computer.
    \item Actually to have true multitasking, the applications run on a machine with multiple processors.
    \item Multitasking results in effective and simultaneous utilization of various system resources such as processors, disks, and printers.
\end{itemize}
\section{Parallel Programming vs Sequential Programming}
\begin{itemize}
    \item In parallel programming, multiple tasks are executed simultaneously, allowing for better performance and maximum utilization of system resources such as processors, disks, and printers.
    \item Sequential Programming means that process are executed sequentially, one after another. When running a sequential Java program, commands are executed linearly, where each process must complete before the next one starts.
\end{itemize}

\section{The operating system multitasking}
The operating system supports multitasking in a \textbf{cooperative} or \textbf{preemptive} manner.
\subsection{Cooperative manner}
In cooperative multitasking each application is responsible for relinquishing control to the processor to enable it to execute the other application, as in earlier versions of operating systems.
\subsection{Preemptive manner}
In the preemptive type multitasking, the processor is responsible for executing each application in a certain amount of time called a time slice, as in modern operating systems.
\subsection{Cooperative VS Preemptive}
table (\ref{tab:cooperativeVsPreemptive}) shows the main differences between the two manners\\
\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|p{0.6\linewidth}|p{0.20\linewidth}|}
        \hline
        Manner                       & responsibility                                                                                                                   & used in                               \\
        \hline
        \multirow{2}{*}{Cooperative} & \textbf{application} $\rightarrow$ relinquishing control to the \textbf{processor} to enable it to execute the other application & earlier versions of operating systems \\
        \hline
        \multirow{2}{*}{Preemptive}  & \textbf{processor} $\rightarrow$ executing each application in a certain amount of time called a  \textbf{time slice}            & modern operating systems              \\
        \hline
    \end{tabular}
    \label{tab:cooperativeVsPreemptive}
    \caption{Cooperative VS Preemptive}
\end{table}
\\
Note: A single processor computer is shared among multiple applications with preemptive multitasking, the processor is switching between the applications at intervals of milliseconds, you feel that all applications run concurrently.
\section{Concepts: Process and Thread}
% \begin{table}[h]
\begin{longtable}{c|p{0.45\linewidth}|p{0.45\linewidth}}
                                    & \Large{Process}                                                                                        & \Large{Thread}                                                                                                                                                    \\
    \hline
    \rotatebox{-90}{Definition}     & \begin{itemize}
                                          \item A process is a program in execution
                                          \item A process is sometime referred as task
                                          \item A process is a collection of one or more threads and associated system resources.
                                          \item A process may be divided into a number of independent units known as threads
                                          \item A process may have a number of threads in it.
                                      \end{itemize}                & \begin{itemize}
                                                                         \item Threads are light-weight processes within a process
                                                                         \item A thread is a dispatchable unit of work
                                                                         \item A thread may be assumed as a subset of a process.
                                                                         \item A thread is a smallest part of the process that can execute concurrently with other parts(threads) of the process
                                                                     \end{itemize}                                                                                                                      \\
    \hline
    \rotatebox{-90}{Multitasking }  &
    \begin{itemize}
        \item Multitasking of two or more processes is known as  \textbf{process-based multitasking}
        \item Process-based multitasking is totally controlled by the \textbf{operating system}
    \end{itemize}
                                    &
    \begin{itemize}
        \item Multitasking of two or more threads is known as \textbf{thread-based multitasking}
        \item The concept of multithreading in a programming language refers to thread-based multitasking
        \item thread-based multitasking can be controlled by the \textbf{programmer} to some extent in a program
    \end{itemize}
    \\
    \hline
    \rotatebox{-90}{Address Space } & A process has its own address space                                                                    & A thread uses the process’s address space and share it with the other threads of that process                                                                     \\
    \hline
    \rotatebox{-90}{Communication } & A process can communicate with other process by using inter-process communication                      & \begin{itemize}
                                                                                                                                                   \item A thread can communicate with other thread (of the same process) directly by using methods like wait(), notify(), notifyAll().
                                                                                                                                                   \item All threads within a process share the same state and same memory space, and can communicate with each other directly, because they share the same variables
                                                                                                                                               \end{itemize} \\
    \hline
    \rotatebox{-90}{New Creation }  & the creation of new processes require duplication of the parent process                                & New threads are easily created                                                                                                                                    \\
    \hline
    \rotatebox{-90}{Control }       & A process does not have control over the sibling process, it has control over its child processes only & Threads have control over the other threads of the same process                                                                                                   \\
    \hline
    \rotatebox{-90}{Construct }     & Processes are an architectural construct                                                               & Thread is a coding construct that does not affect the architecture of an application
\end{longtable}
% \caption{Process VS Thread}
% \end{table}
\subsection{Process containing single and multiple threads}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/process-containing-threads.png}
\end{figure}
\subsection{The advantages of thread-based multitasking as compared to process-based multitasking :}
\begin{itemize}
    \item Threads share the same address space.
    \item  Context-switching between threads is normally inexpensive.
    \item Communication between threads is normally inexpensive.
    \item Java supports thread-based multitasking.
\end{itemize}
\section{Context Switching}
\begin{itemize}
    \item The concept of context switching is integral to threading.
    \item A hardware timer is used by the processor to determine the end of the time-slice for each thread.
    \item The timer signals at the end of the timeslice and in turn the processor saves all information required for the current thread onto a \textbf{stack}. Then the processor moves this information from the stack into a predefined data structure called a \textbf{context structure}.
    \item When the processor wants to switch back to a previously executing thread, it transfers all the information from the context structure associated with the thread to the stack.
\end{itemize}

\chapter{THREADS IN JAVA}
We focus on learning how to write an application containing multiple tasks that can be executed concurrently. In Java, this is realized by using multithreading techniques.\\
\section{Threads}
\begin{itemize}
    \item All threads within a process share the same state and same memory space, and can communicate with each other directly, because they share the same variables.
    \item A single process might contain multiple threads.
    \item Java supports thread-based multitasking
    \item Threads are lightweight processes as the overhead of switching between threads is less
    \item They can be easily spawned
    \item The Java Virtual Machine (JVM) spawns a thread when your program is run called the Main Thread
    \item Multiple Threads on single CPU or multiple CPUs:\begin{figure}[!h]
              \includegraphics[width=0.4\linewidth]{figures/Multiple-threads-on-multiple-CPUs.png}
              Multiple Threads on multiple CPUs
              \\ ----------------------------------------------------------------------- \\
              \includegraphics[width=0.4\linewidth]{figures/Multiple-threads-sharing-a-single-CPU.png}
              Multiple threads sharing a single CPU
          \end{figure}
    \item Program with master and children threads \begin{figure}[!h]
              \centering
              \includegraphics[width=0.6\linewidth]{figures/Program-with-master-and-children-threads.png}
          \end{figure}
\end{itemize}
\subsection{Why do we need threads?}
\begin{itemize}
    \item To  enhance parallel processing
    \item To increase response to the user
    \item To utilize the idle time of the CPU
    \item Prioritize your work depending on priority
\end{itemize}
\section{Implementing Threads in Java}
\begin{itemize}
    \item Threads are objects in the Java language. They can be created by using two different mechanisms:
          \begin{enumerate}
              \item Create a class that \textbf{extends} the standard \textbf{Thread} class.
              \item Create a class that \textbf{implements} the standard \textbf{Runnable} interface
          \end{enumerate}
          \begin{figure}[!h]
              \centering
              \includegraphics[width=0.8\linewidth]{figures/threads-in-java.png}
          \end{figure}

    \item Thread can be defined by:
          \begin{itemize}
              \item Extending the java.lang.Thread class, or
              \item Implementing the java.lang.Runnable interface.
          \end{itemize}
    \item The run() method should be overridden and should contain the code that will be executed by the new thread.
          This method must be public with a void return type and should not take any arguments.
    \item run() method is the starting point for thread execution
\end{itemize}
\subsection{Extending the Thread Class}
\begin{enumerate}
    \item  Create a class by extending the Thread class and override the run() method:
          \begin{lstlisting}[language=java]
class MyThread extends Thread {
    public void run() {
	    // thread body of execution
    }
}
    \end{lstlisting}
    \item Create a thread object:
          \begin{lstlisting}[language=java]
MyThread thr1 = new MyThread();
    \end{lstlisting}
    \item Start Execution of created thread: \begin{lstlisting}[language=java]
thr1.start();
    \end{lstlisting}
\end{enumerate}
\textbf{Example} \\
\begin{lstlisting}[language=java]
/* ThreadEx1.java: A simple program creating and invoking a thread object
by extending the standard Thread class. */
class MyThread extends Thread {
    public void run() {
        System.out.println(" this thread is running ... ");
    }
}
class ThreadEx1 {
    public static void main(String [] args ) {
        MyThread t = new MyThread();
        t.start();
    }
}
\end{lstlisting}
\subsection{Implementing the Runnable Interface}
It is more preferred to implement the Runnable Interface so that we can extend properties from other classes
\begin{enumerate}
    \item Create a class that implements the interface Runnable and override run() method:
          \begin{lstlisting}[language=java]
class MyThread implements Runnable {
    ...
    public void run() {
        // thread body of execution
    }
}
    \end{lstlisting}
    \item Creating Object:
          \begin{lstlisting}[language=java]
        MyThread myObject = new MyThread();
    \end{lstlisting}
    \item Creating Thread Object:
          \begin{lstlisting}[language=java]
        Thread thr1 = new Thread(myObject);
    \end{lstlisting}
    \item Start Execution:
          \begin{lstlisting}[language=java]
        thr1.start();
    \end{lstlisting}
\end{enumerate}
\textbf{Example} \\
\begin{lstlisting}[language=java]
/* ThreadEx2.java: A simple program creating and invoking a thread object
by implementing Runnable interface. */
class MyThread implements Runnable {
    public void run() {
        System.out.println(" this thread is running ... ");
    }
}
class ThreadEx2 {
    public static void main(String [] args ) {
        Thread t = new Thread(new MyThread());
        t.start();
    }
}
\end{lstlisting}
\section{Life cycle of threads \& Thread states}
\subsection{Life Cycle of Thread}
% \begin{figure}[!h]
% \centering
\includegraphics[width=0.9\linewidth]{figures/Life-cycle-of-Java-threads.png}
% \end{figure}
\subsection{Thread States}
% \begin{figure}[!h]
% \centering
\includegraphics[width=0.9\linewidth]{figures/thread-states.png}
% \end{figure}

A thread can be in one of five states: New, Ready, Running, Blocked, or Finished showed in figure(\ref{fig:thread-states-detaild}).\\
% \begin{figure}[!h]
% \centering
\includegraphics[width=0.9\linewidth]{figures/thread-states-detaild.png}
\label{fig:thread-states-detaild}
% \caption{Thread states}
% \end{figure}

\subsection{Thread termination}
A thread becomes Not Runnable when one of these events occurs:
\begin{itemize}
    \item  Its sleep method is invoked.
    \item  The thread calls the wait method to wait for a specific condition to be satisfied.
    \item  The thread is blocking on I/O.
\end{itemize}
\section{Good example}
\begin{itemize}
    \item Consider a simple web server
    \item    The web server listens for request and serves it
    \item If the web server was not multithreaded, the requests processing would be in a queue, thus increasing the response time and also might hang the server if there was a bad request.
    \item By implementing in a multithreaded environment, the web server can serve multiple request simultaneously thus improving response time
\end{itemize}
\section{Running threads}
\begin{lstlisting}[language=java]
class mythread implements Runnable{
    public void run(){
        System.out.println("Thread Started");
    }
}

class mainclass {
    public static void main(String args[]){
        Thread  t = new Thread(new mythread()); // This is the way to instantiate a 					 thread implementing runnable interface
        t.start(); // starts the thread by running the run method
    }
}
\end{lstlisting}
\begin{itemize}
    \item Calling t.run() does not start a thread, it is just a simple method call.
    \item Creating an object does not create a thread, calling start() method creates the thread.
\end{itemize}

\section{Synchronization}
\begin{itemize}
    \item Synchronization is prevent data corruption
    \item Synchronization allows only one thread to perform an operation on a object at a time.
    \item If multiple threads require an access to an object, synchronization helps in maintaining consistency.
\end{itemize}
\subsection{Example on Synchronization}
\begin{lstlisting}[language=java]
public class Counter{
    private int count = 0;
    public int getCount(){
        return count;
    }

    public void setCount(int count){
        this.count = count;
    }
}
\end{lstlisting}
\begin{itemize}
    \item In this example, the counter tells how many an access has been made.
    \item If a thread is accessing setCount and updating count and another thread is accessing getCount at the same time, there will be inconsistency in the value of count.
\end{itemize}
\subsubsection{Fixing the example}
\begin{lstlisting}[language=java]
public class Counter{
	private static int count = 0;
	public synchronized int getCount(){
		return count;
	}

	public synchronized void setCount(int count){
    	this.count = count;
	}
}
\end{lstlisting}
\begin{itemize}
    \item By adding the synchronized keyword we make sure that when one thread is in the setCount method the other threads are all in waiting state.
\end{itemize}
\subsection{What about static methods?}
\begin{lstlisting}[language=java]
public class Counter{
	private int count = 0;
	public static synchronized int getCount(){
	    return count;
	}

	public static synchronized void setCount(int count){
	    this.count = count;
	}
}
\end{lstlisting}
\begin{itemize}
    \item In this example the methods are static and hence are associated with the class and not the instance.
    \item Hence the lock is placed on the class object that is, Counter.class object and not on the object itself.  Any other non static synchronized methods are still available for access by other threads.
\end{itemize}
\subsection{Common Synchronization mistake}
\begin{lstlisting}[language=java]
public class Counter{
	private int count = 0;
	public static synchronized int getCount(){
		 return count;
	}

	public synchronized void setCount(int count){
		this.count = count;
	}
}
\end{lstlisting}
\begin{itemize}
    \item The common mistake here is one method is static synchronized and another method is non static synchronized.
    \item This makes a difference as locks are placed on two different objects. The class object and the instance and hence two different threads can access the methods simultaneously.
\end{itemize}
\subsection{Synchronization vs Static Synchronization}

\begin{tabular}{c|p{0.35\linewidth}|p{0.35\linewidth}}
    Feature                    & \textbf{Synchronization}                                                                & \textbf{Static Synchronization}                                                                     \\
    \hline
    Scope                      & Object-level                                                                            & Class-level                                                                                         \\
    \hline
    Lock Acquisition           & Acquires lock on the object instance                                                    & Acquires lock on the class itself                                                                   \\
    \hline
    Impact on Multiple Objects & Each object's synchronized methods can be accessed by different threads simultaneously. & Only one thread can access any static synchronized method of the class at a time.                   \\
    \hline
    Usage                      & Used to protect shared resources at the object level                                    & Used to protect shared resources at the class level or for operations that involve the class itself \\
    \hline
    Declaration                & Applied to instance methods using the synchronized keyword                              & Applied to static methods using the static synchronized keywords                                    \\
    \hline
    Example                    & public synchronized void deposit() { ... }                                              & public static synchronized void getInstance() { ... }                                               \\
\end{tabular}

\section{Object locking}
\begin{itemize}
    \item The object can be explicitly locked in this way
          \begin{lstlisting}[language=java]
        synchronized(myInstance){
            try{
            wait();
            }catch(InterruptedException ex){
                System.out.println("Iam in this ");
                notifyAll();
            }
            \end{lstlisting}
    \item The synchronized keyword locks the object. The wait keyword waits for the lock to be acquired, if the object was already locked by another thread. notifyall() notifies other threads that the lock is about to be released by the current thread.
    \item Another method notify() is available for use, which wakes up only the next thread which is in queue for the object, notifyall() wakes up all the threads and transfers the lock to another thread having the highest priority.
\end{itemize}

\section{Thread Priority}
\begin{itemize}
    \item In Java, each thread is assigned priority, which affects the order in which it is scheduled for running. The threads so far had same default priority (NORM\_PRIORITY) and they are served using FCFS policy.

    \item Java allows users to change priority:
          \begin{lstlisting}[language=java]
            ThreadName.setPriority(intNumber)
                \end{lstlisting}
          \begin{itemize}
              \item MIN\_PRIORITY = 1
              \item NORM\_PRIORITY = 5
              \item MAX\_PRIORITY = 10
          \end{itemize}
\end{itemize}

\section{Shared Resources}
\begin{itemize}
    \item If one thread tries to read the data and other thread tries to update the same data, it leads to inconsistent state.
    \item This can be prevented by synchronizing access to the data.
    \item Use “synchronized” method:
          \begin{lstlisting}[language=java]
public synchronized void update()
{
    ....
}
            \end{lstlisting}
\end{itemize}
\subsection{the driver: 3 Threads sharing the same object}
\begin{lstlisting}[language=java]
class InternetBankingSystem {
    public static void main(String [] args  ) {
        Account accountObject = new Account (); 
        Thread t1 = new Thread(new MyThread(accountObject));
            Thread t2 = new Thread(new YourThread(accountObject));
            Thread t3 = new Thread(new HerThread(accountObject));
        t1.start();
        t2.start();
        t3.start();
        // DO some other operation
    } // end main()
}
\end{lstlisting}
\subsection{Shared account object between 3 threads}
\begin{lstlisting}[language=java]
class MyThread implements Runnable  {
 Account account;
        public MyThread (Account s) {  account = s;}
        public void run() { account.deposit(); }
} // end class MyThread

class YourThread implements Runnable  {
 Account account;
        public YourThread (Account s) { account = s;}
        public void run() { account.withdraw(); }
} // end class YourThread

class HerThread implements Runnable  {
 Account account;
        public HerThread (Account s) { account = s; }
        public void run() {account.enquire(); }
} // end class HerThread
    \end{lstlisting}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.3\linewidth]{figures/three-access-one.png}
\end{figure}

\subsection{Monitor (shared object access): serializes operation on shared objects}
\begin{lstlisting}[language=java]
class Account {   // the 'monitor'
    int balance;

 // if 'synchronized' is removed, the outcome is unpredictable
  public synchronized void deposit( ) {
     // METHOD BODY : balance += deposit_amount;
   }

    public synchronized void withdraw( ) {
      // METHOD BODY: balance -= deposit_amount;
    }
    public synchronized void enquire( ) {
      // METHOD BODY: display balance.
    }
 }
\end{lstlisting}

\section{Threaded Applications}
\subsection{Multithreaded Web Server: For Serving Multiple Clients Concurrently}
\includegraphics[width=0.5\linewidth]{figures/Web Server.png}
\subsubsection{Architecture for Multithread Servers}
\begin{itemize}
    \item Multithreading enables servers to maximize their throughput, measured as the number of requests processed per second.
    \item Threads may need to treat requests with varying priorities:
          \begin{itemize}
              \item A corporate server could prioritize request processing according to class of customers.
          \end{itemize}
    \item Architectures:
          \begin{itemize}
              \item Worker pool
              \item Thread-per-request
              \item Thread-per-connection
              \item Thread-per-object
          \end{itemize}
\end{itemize}

\subsection{Internet Browser + Youtube}
\includegraphics[width=0.5\linewidth]{figures/youtube.png}

\subsection{Editing and Printing documents in background}
\includegraphics[width=0.5\linewidth]{figures/editing-and-print.png}
\subsection{Multithreaded/Parallel File Copy}
\includegraphics[width=0.7\linewidth]{figures/file-copy.png}

\subsection{Online Bank: Serving Many Customers and Operations}
\includegraphics[width=0.5\linewidth]{figures/online-banking.png}






















\chapter{Parallel Computing: Overview (PSC)}{by John Urbanicurbanic@psc.edu}
\section{Why we need parallel computing?}
for \textbf{New Applications} \\
The graph shows that applications that require processing large amounts of data are the ones that benefit most from parallel computing.
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/new-applications.png}
\end{figure}
\begin{itemize}
    \item The performance capabilities of supercomputers are expressed using a standard rate for indicating the number of floating-point arithmetic calculations systems can perform on a per-second basis. The rate, \textbf{floating-point operations per second}, is abbreviated as \textbf{FLOPS}.
    \item The per-second rate "FLOPS" is commonly misinterpreted as the plural form of "FLOP" (short for "floating-point operation") \cite{flops}
\end{itemize}
\section{Clock Speed}
\subsection{clock speed over previous years}
\begin{figure}[!h]
    \centering
    \subfloat{\includegraphics[width=0.55\linewidth]{figures/clock-speeds01.png}}
    \subfloat{\includegraphics[width=0.55\linewidth]{figures/clock-speeds02.png}}
\end{figure}

\subsection{Y-MP vs C90 supercomputers}
When the PSC \footnote{PSC ~ Pittsburgh Supercomputing Center, The Pittsburgh Supercomputing Center (PSC) is a high performance computing and networking center founded in 1986 and one of the original five NSF Supercomputing Centers.} went from a 2.7 GFlop Y-MP
to a 16 GFlop C90, the clock only got 50\%
faster. The rest of the speed increase was
due to increased use of parallel techniques:
\begin{itemize}
    \item More processors (8 $\rightarrow$ 16)
    \item Longer vector pipes (64 $\rightarrow$ 128)
    \item Parallel functional units (2)
\end{itemize}

\begin{table}[h]
    \centering
    \begin{tabular}{l|c|c}
        --                        & Y-MP & C90 \\
        \hline
        processors                & 8    & 16  \\
        \hline
        vector pipes              & 64   & 128 \\
        \hline
        Parallel functional units & 2    & 2   \\
    \end{tabular}
\end{table}

So, we want as many processors working
together as possible. How do we do this?
There are two distinct elements:
\begin{itemize}
    \item Hardware: vendor does this
    \item Software: you, at least today
\end{itemize}

\section{Amdahl's Law}
How many processors can we really use? \\

\begin{minipage}{0.6\linewidth}
    Let's say we have a legacy
    code such that is it only
    feasible to convert half of
    the heavily used routines
    to parallel:
    \begin{itemize}
        \item If we run this on a parallel
              machine with five processors:
              Our code now takes about
              60s. We have sped it up
              by about 40\%.
        \item Let's say
              we use a thousand
              processors:
              We have now sped our code
              by about a factor of two.
    \end{itemize}
\end{minipage}
\hfill
\begin{minipage}{0.3\linewidth}
    \centering
    \begin{tabular}{|c|c|}
        \hline
        25s & Serial   \\
        \hline
        50s & Parallel \\
        \hline
        25s & Serial   \\
        \hline
    \end{tabular}
    \\
    \includegraphics[width=0.99\linewidth]{figures/amdahl-law02.png}
\end{minipage}
% \begin{minipage}{0.6\linewidth}

% \end{minipage}
% \hfill
% \begin{minipage}{0.4\linewidth}
% \end{minipage}

This seems pretty depressing, and it does point out one limitation of converting old
codes one subroutine at a time. However, most new codes, and almost all parallel
algorithms, can be written almost entirely in parallel (usually, the “start up” or
initial input I/O code is the exception), resulting in significant practical speed ups.
This can be quantified by how well a code scales which is often measured as
efficiency.
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/amdahl-law03.png}
\end{figure}
\subsection{Amdahl's Law equation}
\subsubsection{Time}
if single-processor finishes one program in one unit of time, how much time will multiple-processors require to finish that task?\\
\begin{equation}
    Time_{for\ 1\ processor} = 1
\end{equation}
\begin{equation}
    Time_{for\ 2\ processor} = \frac{1}{2}
\end{equation}
\begin{equation}
    Time_{for\ n\ processors} = \frac{1}{n}
\end{equation}
\subsubsection{Speedup}
If you are using n processors, your $Speedup_n$ is:
\begin{equation}
    Speedup_n = \frac{T_1}{T_n}
\end{equation}
And your Speedup $\mathrm{Efficiency_n}$ is:
\begin{equation}
    \mathrm{Efficiency_n} = \frac{Speedup_n}{n}
\end{equation}
which could be as high as 1., but probably never will be.
\subsubsection{Amdahl's law}
If you put in n processors, you should get n times Speedup
(and 100\% Speedup Efficiency), right? Wrong!
There are always some fraction of the total operation that is inherently
sequential and cannot be parallelized no matter what you do. This includes
reading data, setting up calculations, control logic, storing results, etc. \\ \\
If you think of all the operations that a program needs to do as being
divided between a fraction that is parallelizable and a fraction that isn't
(i.e., is stuck at being sequential), then Amdahl's Law says:

\begin{equation}
    Speedup_n = \frac{T_1}{T_n} = \frac{1}{\frac{F_{parallel}}{n}+F_{sequential}} = \frac{1}{\frac{F_{parallel}}{n}+(1-F_{parallel})}
\end{equation}

\subsubsection{Maximum Possible SpeedUp}
\begin{equation}
    max\ Speedup = \frac{1}{1-F_{parallel}}
\end{equation}
\subsubsection{Example 1}
5\% of a parallel programs's execution time is spent within inherently sequential code.
Calculate The maximum speedup achievable by this program, regardless of how many PEs are used \\
Solution \\
\begin{equation}
    Speedup_\infty = \frac{1}{\frac{0.95}{\infty}+0.05} = 20
\end{equation}
\subsubsection{Example 2}
95\% of a program's execution time occurs inside a loop
that can be executed in parallel. What is the maximum
speedup we should expect from a parallel version of the
program executing on 8 CPUs? \\
Solution \\
\begin{equation}
    Speedup_8 = \frac{1}{\frac{0.95}{8}+0.05} \approx 5.9
\end{equation}
\section{Shared Memory and Distributed Memory}
\subsection{Shared Memory}
\begin{minipage}{0.65\linewidth}
    Easiest to program. There are no
    real data distribution or
    communication issues. Why
    doesn't everyone use this
    scheme?
    \begin{itemize}
        \item Limited numbers of processors
              (tens) - Only so many
              processors can share the same
              bus before conflicts dominate.
        \item Limited memory size -
              Memory shares bus as well.
              Accessing one part of memory
              will interfere with access to
              other parts.
    \end{itemize}
\end{minipage}
\hfill
\begin{minipage}{0.3\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/shared-memory01.png}
    \includegraphics[width=0.99\linewidth]{figures/shared-memory02.png}
\end{minipage}

\subsection{Distributed Memory}
\begin{itemize}
    \item Number of processors only limited by physical size (tens of meters).
    \item Memory only limited by the number of processors time the maximum memory per processor (very large). However, physical packaging usually dictates no local disk per node and hence no virtual memory.
    \item Since local and remote data have much different access times, data distribution is very important. We must minimize communication.
\end{itemize}

\begin{table}[h!]
    \centering
    \caption{Comparison of Shared and Distributed Memory Architectures}
    \label{tab:shared_distributed}
    \begin{tabular}{|l|c|c|}
        \hline
        \textbf{Feature}     & \textbf{Shared Memory} & \textbf{Distributed Memory}                   \\ \hline
        Ease of Programming  & Easier                 & More difficult                                \\ \hline
        Data Distribution    & Not required           & Crucial                                       \\ \hline
        Communication Issues & Minimal                & Significant                                   \\ \hline
        Number of Processors & Limited (tens)         & Limited by physical size (tens of meters)     \\ \hline
        Memory Size          & Limited                & Very large                                    \\ \hline
        Local Disk per Node  & Often available        & Usually not available                         \\ \hline
        Virtual Memory       & Supported              & Not typically supported                       \\ \hline
        Data Access Times    & Uniform                & Dependent on data location (local vs. remote) \\ \hline
    \end{tabular}
\end{table}

\subsubsection{Common Distributed Memory Machines}
\begin{minipage}{0.5\linewidth}
    \begin{itemize}
        \item CM-2
        \item CM-5
        \item T3E
    \end{itemize}
\end{minipage}
\begin{minipage}{0.5\linewidth}
    \begin{itemize}
        \item Workstation Cluster
        \item  SP3
        \item  TCS
    \end{itemize}
\end{minipage}
\subsection{Common parallel computer architectures \cite{SIMD-and-MIMD}}
\begin{itemize}
    \item \textbf{SIMD} and \textbf{MIMD} are types of computer architectures that are used to improve the performance of certain types of computational tasks.
    \item The basis of this classification is the number of data and instruction streams.
    \item \textbf{SIMD}, short for Single Instruction Multiple Data, computer architecture can execute a single instruction on multiple data streams.
    \item On the other hand, the \textbf{MIMD} (Multiple Instruction Multiple Data) computer architectures can execute several instructions on multiple data streams.
    \item While the CM-2 is SIMD (one instruction unit for multiple processors), all
          the new machines are MIMD (multiple instructions for multiple
          processors) and based on commodity processors.\\
          \begin{minipage}{0.5\linewidth}
              \begin{itemize}
                  \item SP-2
                  \item CM-5
                  \item T3E
                  \item Workstations
                  \item TCS
              \end{itemize}
          \end{minipage}
          \begin{minipage}{0.5\linewidth}
              \begin{itemize}
                  \item POWER2
                  \item SPARC
                  \item Alpha
                  \item Your Pick
                  \item Alpha
              \end{itemize}
          \end{minipage}
    \item Therefore, the single most defining characteristic of any of these machines
          is probably the network.
\end{itemize}
\section{Networking for distributed machines}
Even with the "perfect" network we have here, performance is determined by two more quantities
that, \textbf{together with the topologies} we'll look at, pretty much define the network: \textbf{latency} and
\textbf{bandwidth}.

\begin{itemize}
    \item latency
    \item bandwidth
    \item topologies
\end{itemize}
\subsection{Latency}
\begin{itemize}
    \item Latency can nicely be defined as the time required to send a message with 0 bytes of data.
    \item This number often reflects either: \begin{itemize}
              \item the overhead of packing your data into packets,
              \item or the delays in making intervening hops across the network between two nodes that aren't next to each other.
          \end{itemize}
\end{itemize}
\subsection{Bandwidth}
\begin{itemize}
    \item Bandwidth is the rate at which very large packets of information can be sent.
    \item If there was no latency, this is the rate at which all data would be transferred.
    \item It often reflects the physical capability of the wires and electronics connecting nodes.
\end{itemize}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/Latency and Bandwidth.png}
\end{figure}
\subsection{Topologies}
\subsubsection{Token-Ring/Ethernet with Workstations}
% \begin{figure}[!h]
% \centering
\includegraphics[width=0.5\linewidth]{figures/Token-Ring.png}
% \end{figure}
\subsubsection{Complete Connectivity}
% \begin{figure}[!h]
% \centering
\includegraphics[width=0.5\linewidth]{figures/Complete Connectivity.png}
% \end{figure}
\subsubsection{Super Cluster / SP2}
% \begin{figure}[!h]
% \centering
\includegraphics[width=0.5\linewidth]{figures/Super Cluster (SP2).png}
% \end{figure}
\subsubsection{CM-2}
% \begin{figure}[!h]
% \centering
\includegraphics[width=0.5\linewidth]{figures/CM-2.png}
% \end{figure}
\subsubsection{Binary Tree}
% \begin{figure}[!h]
% \centering
\includegraphics[width=0.5\linewidth]{figures/Binary Tree.png}
% \end{figure}
\subsubsection{CM-5 Fat Tree}
% \begin{figure}[!h]
% \centering
\includegraphics[width=0.5\linewidth]{figures/CM-5 Fat Tree.png}
% \end{figure}
\subsubsection{INTEL Paragon (2-D Mesh)}
% \begin{figure}[!h]
% \centering
\includegraphics[width=0.5\linewidth]{figures/INTEL Paragon (2-D Mesh).png}
% \end{figure}
\subsubsection{3-D Torus}
\begin{minipage}{0.4\linewidth}
    % \begin{figure}[h!]
    % \centering
    \includegraphics[width=0.9\linewidth]{figures/3-D Torus.png}
    % \end{figure}
\end{minipage}
\begin{minipage}{0.4\linewidth}
    \begin{itemize}
        \item T3E has Global
              Addressing hardware,
              and this helps to
              simulate shared
              memory.
        \item Torus means that “ends”
              are connected. This
              means A is really
              connected to B and the
              cube has no real
              boundary.
    \end{itemize}
\end{minipage}
\subsubsection{TCS Fat Tree}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/TCS Fat Tree.png}
\end{figure}

\section{Data Parallel vs Work Sharing}
% \begin{table}[h!]
\begin{longtable}{|c|p{0.43\linewidth}|p{0.43\linewidth}|}
    \hline
    --            & \textbf{Data Parallel}                                                                                 & \textbf{Work Sharing}                                                                                 \\
    \hline
    --            & Only one executable                                                                                    & Splits up tasks (as opposed to arrays in date parallel) such as loops amongst separate processors     \\
    \hline
    computation   & Do computation on arrays of data using array operators                                                 & Do computation on loops that are automatically distributed                                            \\
    \hline
    communication & Do communications using array shift or rearrangement operators.                                        & Do communication as a side effect of data loop distribution. Not important on shared memory machines. \\
    \hline
    Good for      & Good for problems with static load balancing that are array-oriented SIMD machines                     & Good for shared memory implementations.                                                               \\
    \hline
    Strengths     & \begin{enumerate}
                        \item Scales transparently to different size machines
                        \item Easy debugging, as there is only one copy of the code executing in a highly synchronized fashion
                    \end{enumerate} & \begin{enumerate}
                                          \item Directive based, so it can be added to existing serial codes
                                      \end{enumerate}                                                                                                          \\
    \hline
    Weaknesses    & \begin{enumerate}
                        \item Much wasted synchronization
                        \item Difficult to balance load
                    \end{enumerate}                                                                      & \begin{enumerate}
                                                                                                               \item Limited flexibility
                                                                                                               \item Efficiency dependent upon structure of existing serial code
                                                                                                               \item May be very poor with distributed memory.
                                                                                                           \end{enumerate}                                                        \\
    \hline
    Variants      & \begin{itemize}
                        \item FORTRAN 90
                        \item CM FORTRAN
                        \item HPF
                        \item   C*
                        \item CRAFT
                    \end{itemize}                                                                                       & \begin{itemize}
                                                                                                                              \item CRAFT
                                                                                                                              \item Multitasking
                                                                                                                          \end{itemize}                                                                                        \\
    \hline
    When to use   & When to use Data Parallel: \begin{itemize}
                                                   \item Very array-oriented programs
                                                         \begin{itemize}
                  \item FEA
                  \item Fluid Dynamics
                  \item Neural Nets
                  \item Weather Modeling
              \end{itemize}
                                                   \item Very synchronized operations
                                                         \begin{itemize}
                  \item Image processing
                  \item Math analysis
              \end{itemize}
                                               \end{itemize}                                                             & When to use Work Sharing: \begin{itemize}
                                                                                                                                                         \item Very large / complex / old existing codes: Gaussian 90
                                                                                                                                                         \item Already multitasked codes: Charmm
                                                                                                                                                         \item Portability (Directive Based)
                                                                                                                                                         \item (Not Recommended)
                                                                                                                                                     \end{itemize}                   \\
    \hline
\end{longtable}
% \end{table}
% \section{Work Sharing}

\subsection{Data Parallel - Data Movement in FORTRAN 90}
\includegraphics[width=0.5\linewidth]{figures/Data Movement in FORTRAN 90-01.png}
\includegraphics[width=0.5\linewidth]{figures/Data Movement in FORTRAN 90-02.png}

\subsection{Work Sharing - CRAYs}
If you have used CRAYs before, this of this as “advanced multitasking”\\
\textbf{Explain:} if you have used CRAYs before, you can think of work sharing as "advanced multitasking." This is because work sharing allows you to split up tasks and distribute them among multiple processors, which can significantly improve the performance of your code.
\section{Load Balancing}
\begin{itemize}
    \item An important consideration which can be controlled by communication is load balancing:\\
          Consider the case where a dataset is distributed evenly over 4 sites.\\
          Each site will run a piece of code which uses the data as input and
          attempts to find a convergence.\\
          It is possible that the data contained at sites 0, 2, and 3 may converge much faster than the data at site 1.
          \\If this is the case, the three sites which finished first will remain \textbf{idle} while site 1 finishes.
          \\When attempting to balance the amount of work being done at each site, one must take into account:
          \begin{itemize}
              \item the speed of the processing site,
              \item the communication "expense" of starting and coordinating separate pieces of work,
              \item  and the amount of work required by various pieces of data.
          \end{itemize}
    \item There are two forms of load balancing: static and dynamic.\\
          \begin{center}
              \begin{forest}
                  [load balancing forms [static load balancing] [dynamic load balancing [task-oriented] [data-oriented]]]
              \end{forest}
          \end{center}
\end{itemize}
\subsection{Static Load Balancing}
\begin{itemize}
    \item In static load balancing, the programmer must make a decision and assign a \textbf{fixed amount} of work to each processing site a priori.
    \item Static load balancing can be used in either the
          \begin{itemize}
              \item  Master-Slave (Host-Node) programming model
              \item or the "Hostless" programming model.
          \end{itemize}
    \item Static Load Balancing yields \textbf{Good performance} when:
          \begin{itemize}
              \item homogeneous cluster
              \item each processing site has an equal amount of work
          \end{itemize}
    \item \textbf{Poor performance} when:
          \begin{itemize}
              \item heterogeneous cluster where some processors are much faster (unless this is taken into account in the program design)
              \item work distribution is uneven
          \end{itemize}
\end{itemize}
\subsection{Dynamic Load Balancing}
\begin{itemize}
    \item Dynamic load balancing can be further divided into the categories:
          \begin{itemize}
              \item \textbf{task-oriented (commonly used form)} \\
                    when one processing site finishes its task, it is assigned another task (this is the most commonly used form).
              \item \textbf{data-oriented (more complicated)}\\
                    when one processing site finishes its task before other sites, the site with the most work gives the idle site some of its data to process (this is much more complicated because it requires an extensive amount of bookkeeping).
          \end{itemize}
    \item Dynamic load balancing can be used only in the Master-Slave programming model.
    \item ideal for: \begin{itemize}
              \item codes where tasks are large enough to keep each processing site busy
              \item codes where work is uneven
              \item heterogeneous clusters
          \end{itemize}
\end{itemize}
\bibliography{./ref}
\bibliographystyle{unsrt}
\end{document}